{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and Convert to FASTQ\n",
    "\n",
    "Sorts a BAM file and converts to FASTQ.\n",
    "The input file is first checked for paired reads count. If its non zero its treated as a paired-end file.\n",
    "Based on the user flag to split into R1 and R2 the paired file is separated into two fastq file. Else a single fastq file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>20</td><td>application_1674723992507_0096</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hadoop16:8089/proxy/application_1674723992507_0096/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hadoop23:8044/node/containerlogs/container_1674723992507_0096_01_000001/HPV_meta__dhananja\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import pysam\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_full=utils.load_arguments([0,\"-shdfs:///Projects/HPV_meta/Jupyter/Bio_pipeline/settings/settings_benchmark.yml\"])\n",
    "\n",
    "args_full=utils.load_arguments(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : remove WORK PATH entirely and use direct input/output paths from YML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "args=args_full[utils.KEY_SORTCONVERT]\n",
    "\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    inputRoot=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "    inputRoot=args['INPUT_ROOT']\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    OUTPUT_FASTQ=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    OUTPUT_FASTQ=os.path.join(WORK_PATH,args['OUTPUT_FASTQ'])\n",
    "\n",
    "\n",
    "THREADS=str(args['THREADS'])\n",
    "SPACE=utils.SPACE\n",
    "# split separate R1 and R2 if paired file\n",
    "is_split_R1R2=args['SPLIT_PAIRS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_paired_reads(x):\n",
    "    \"\"\"\n",
    "    Count paired  in BAM reads countfile using pysam.\n",
    "    Returns True if non zero.\n",
    "    \"\"\"\n",
    "\n",
    "    o=pysam.view(x, '-c','-f','1',catch_stdout=True)\n",
    "    if int(o)!=0: # if non zero paired reads count\n",
    "        return True\n",
    "\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def isPaired(file):\n",
    "    \"\"\"\n",
    "    Counts paired reads count in BAM using samtools.\n",
    "    Returns True if non zero.\n",
    "    \"\"\"\n",
    "\n",
    "    cmd1=utils.SAMTOOLS+' view -c -f 1 '+file \n",
    "    print(cmd1)\n",
    "    p1=subprocess.Popen(cmd1.split(),stdout=subprocess.PIPE)\n",
    "    cmd2='head -n 1'\n",
    "    p2=subprocess.run(cmd2.split(),stdin=p1.stdout,stdout=subprocess.PIPE)\n",
    "    if p1.stdout  :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def sort(file):\n",
    "    \"\"\"\n",
    "    Sort given BAM file using pysam\n",
    "    \"\"\"\n",
    "    sort_file = utils.SORTED_PREFIX+file\n",
    "    try:\n",
    "        pysam.sort('-@',THREADS,'-m','2G','-n',file,'-o',sort_file,catch_stdout=False)\n",
    "    except pysam.SamtoolsError:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return sort_file\n",
    "\n",
    "\n",
    "\n",
    "def sort_convert_fastq(file_path):\n",
    "    \"\"\"\n",
    "    Map function to run on single BAM file.\n",
    "    First the file is sorted and then the sorted file is converted to\n",
    "    single or paired FASTQ files depending on user argument\n",
    "    and if file is paired or not.\n",
    "    Output is copied back to hdfs.\n",
    "    If output file name is already present in destination the process is skipped\n",
    "    to avoid processing of same file incase of resubmit of failed run.\n",
    "\n",
    "\n",
    "    P.S/TODO: Commented code is to run via samtools and stays due to hadoop18 problem\n",
    "    of running samtools via subprocess.\n",
    "    Once hadoop18 is fixed samtools can be uncommented and pysam code then can be subsequently deleted\n",
    "    \"\"\"\n",
    "    file=os.path.split(file_path)[1]\n",
    "    filename=file.split('.')[0]\n",
    "    output=filename+'.fq.gz'\n",
    "    if not hdfs.exists(os.path.join(OUTPUT_FASTQ,output)): # check if output already exists\n",
    "               \n",
    "    \n",
    "        hdfs.copy_to_local(file_path, overwrite=True)\n",
    "         \n",
    "        out_file_r1=''\n",
    "        out_file_r2=''\n",
    "        out_file=''\n",
    "\n",
    "         # check if bam file has paired reads\n",
    "        try:\n",
    "\n",
    "            isPairedFile=count_paired_reads(file)\n",
    "\n",
    "\n",
    "            print('is paired ? ', isPairedFile)\n",
    "\n",
    "            # sort\n",
    "            print(\"INFO: Run sort \", file)\n",
    "            sort_file=sort(file)\n",
    "            os.remove(file)\n",
    "            os.rename(sort_file,sort_file.split(utils.SORTED_PREFIX)[1])\n",
    "\n",
    "            # if the bam file is paired and user wants to split into separate R1 and R2 fastq\n",
    "            if (isPairedFile and is_split_R1R2):\n",
    "                out_file_r1=filename+utils.R1_SUFFIX_EXTENSION\n",
    "                out_file_r2=filename+utils.R2_SUFFIX_EXTENSION\n",
    "                #params={'fastq':file,'-1':out_file_r1,'-2':out_file_r2, '-0':'/dev/null', '-s':'/dev/null', '-c':1 ,'-@':THREADS}\n",
    "                try:\n",
    "                    pysam.fastq(file,'-1',out_file_r1,'-2',out_file_r2, '-0','/dev/null', '-s','/dev/null', '-c','1','-@',THREADS,'-N',catch_stdout=False)\n",
    "                except pysam.SamtoolsError:\n",
    "                    traceback.print_exc()\n",
    "\n",
    "            else:   # else if the file is single read or the user wants to split into a single fastq\n",
    "                out_file=filename+'.fq.gz'\n",
    "                #params={'fastq': file,'-0': out_file, '-s':'/dev/null', '-c':1, '-@': THREADS}\n",
    "                if isPairedFile: # samtools fastq has two different argument values for output if the file is paired or not\n",
    "                    OUT_ARG='-o'\n",
    "                else :\n",
    "                    OUT_ARG='-0'\n",
    "                try:\n",
    "                    pysam.fastq(file, OUT_ARG, out_file,  '-c','1','-@',THREADS,'-N',catch_stdout=False)\n",
    "                except pysam.SamtoolsError:\n",
    "                    traceback.print_exc()\n",
    "\n",
    "        # convert to fastq command  using samtools\n",
    "        #     cmdConvert=utils.build_command(utils.SAMTOOLS,params)\n",
    "        #     print('INFO: Run sort and convert', cmdConvert)\n",
    "        #     p1 = subprocess.run(cmdConvert.split(), stdout=subprocess.PIPE) # run convert from pipe from sort command\n",
    "\n",
    "            os.remove(file)\n",
    "            if os.path.exists(out_file): # either a single fastq exists or separate R1 R2 fastq files\n",
    "                hdfs.copy_to_hdfs(out_file,OUTPUT_FASTQ,overwrite=True)\n",
    "                os.remove(out_file)\n",
    "                return True\n",
    "            if os.path.exists(out_file_r1):\n",
    "                hdfs.copy_to_hdfs(out_file_r1,OUTPUT_FASTQ,overwrite=True)\n",
    "                hdfs.copy_to_hdfs(out_file_r2,OUTPUT_FASTQ,overwrite=True)\n",
    "                os.remove(out_file_r1)\n",
    "                os.remove(out_file_r2)\n",
    "                return True\n",
    "        except pysam.SamtoolsError:\n",
    "            traceback.print_exc()\n",
    "            utils.hdfs_delete_file(file_path) # delete corrupted input file\n",
    "            return False\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('skipping existing file: ', filename)            \n",
    "        return None\n",
    "\n",
    "        \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input files hdfs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "path hdfs://rpc.namenode.service.consul:8020/Projects/HPV_meta/Study2/run_dir/test2/input not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/hopsdata/tmp/nm-local-dir/usercache/HPV_meta__dhananja/appcache/application_1674723992507_0096/container_1674723992507_0096_01_000001/utils.py\", line 117, in load_file_names\n",
      "    files_list = [d['path'] for d in hdfs.lsl(hdfs_root, recursive=True) if d['kind'] == 'file']\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hops/hdfs.py\", line 465, in lsl\n",
      "    hdfs_path = _expand_path(hdfs_path, project)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hops/hdfs.py\", line 196, in _expand_path\n",
      "    raise IOError(\"path %s not found\" % hdfs_path)\n",
      "OSError: path hdfs://rpc.namenode.service.consul:8020/Projects/HPV_meta/Study2/run_dir/test2/input not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputFiles=utils.load_file_names(inputRoot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'inputFiles' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'inputFiles' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of input files {}\".format(inputFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "finalList = sc.parallelize(inputFiles,sc.getConf().get(\"spark.executor.instances\")).map(sort_convert_fastq).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "print(len(inputFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}