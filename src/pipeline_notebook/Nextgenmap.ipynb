{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Nextgenmap\n",
    "Runs `nextgenmap` in paired and single end setting. Samples are grouped by `R1` and `R2` in file names.\n",
    "If the keyword `R` is not found its treated as a single ended file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>24</td><td>application_1674723992507_0129</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hadoop16:8089/proxy/application_1674723992507_0129/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hadoop8:8044/node/containerlogs/container_1674723992507_0129_01_000001/HPV_meta__dhananja\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "import stat\n",
    "\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import utils\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#args_full=utils.load_arguments([0,\"-shdfs:///Projects/HPV_meta/Jupyter/Bio_pipeline/settings/settings_benchmark.yml\",\"-ihdfs:///Projects/HPV_meta/Ainhoa/input\",\"-ohdfs:///Projects/HPV_meta/Ainhoa/output\"])\n",
    "args_full=utils.load_arguments(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "args=args_full[utils.KEY_NGM]\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    INPUT_ROOT=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "   INPUT_ROOT=os.path.join(WORK_PATH,args['INPUT_ROOT'])\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    OUTPUT_ROOT=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    OUTPUT_ROOT=os.path.join(WORK_PATH,args['OUTPUT_ROOT'])\n",
    "\n",
    "VERY_FAST='--very-fast'\n",
    "REFERENCE_PATH=args['REFERENCE_FILE']\n",
    "MIN_I=str(args['MIN-IDENTITY'])\n",
    "MIN_R=str(args['MIN-RESIDUES'])\n",
    "THREADS=args['THREADS']\n",
    "LOG_DIR=args['LOGS_ROOT']\n",
    "is_very_fast=args['VERY_FAST']\n",
    "IS_INSTALL_NGM=args['INSTALL_NGM']\n",
    "SPACE=utils.SPACE\n",
    "TOOL_PATH='Tools/ngm_built/NGM'\n",
    "if IS_INSTALL_NGM:\n",
    "    TOOL='./NGM/bin/ngm-0.5.5/ngm'\n",
    "else :\n",
    "    TOOL='ngm'\n",
    "\n",
    "\n",
    "def chmod_exec(tool):\n",
    "    st = os.stat(tool)\n",
    "    os.chmod(tool, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "def install_ngm(tool_path):\n",
    "    print('INFO: Installing NGM from path ', tool_path)\n",
    "    hdfs.copy_to_local(tool_path,overwrite=True)\n",
    "    lib1='./NGM/bin/ngm-0.5.5/ngm-core'\n",
    "    lib2='./NGM/bin/ngm-0.5.5/ngm'\n",
    "    chmod_exec(lib1)\n",
    "    chmod_exec(lib2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Map functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### paired files\n",
    "def apply_ngm_paired(x,REFERENCE_PATH):\n",
    "    \"\"\"\n",
    "    Runs NGM in paired mode via subprocess for single R1 R2 pair.\n",
    "    Output is saved in SAM format and copied back to hdfs.\n",
    "    \"\"\"\n",
    "\n",
    "    filename_forward_path=x[0] #r1\n",
    "    filename_reverse_path=x[1] #r2\n",
    "    # split path to get file names\n",
    "    filename_forward=os.path.basename(filename_forward_path)\n",
    "    filename_reverse=os.path.basename(filename_reverse_path)\n",
    "\n",
    "    output_file=filename_forward.split('.')[0].replace(utils.R1,'')\n",
    "    if  utils.skip_file(filename_forward,output_file,OUTPUT_ROOT):\n",
    "        return [-1]\n",
    "\n",
    "    if IS_INSTALL_NGM and not os.path.exists(os.path.basename(TOOL_PATH)):\n",
    "         install_ngm(TOOL_PATH)\n",
    "\n",
    "    # get file name and copy to local\n",
    "    ref=os.path.split(REFERENCE_PATH)[1]\n",
    "    if not os.path.exists(ref):        \n",
    "        hdfs.copy_to_local(REFERENCE_PATH)\n",
    "    hdfs.copy_to_local(filename_forward_path, overwrite=True)\n",
    "    hdfs.copy_to_local(filename_reverse_path, overwrite=True)\n",
    "    \n",
    "\n",
    "    parameters = { '-i':MIN_I, '-R': MIN_R, '-p': utils.EMPTY, '-r': ref, '-1': filename_forward, '-2': filename_reverse,\n",
    "                  '--silent-clip': utils.EMPTY, '-o': output_file, '-t': THREADS, '--no-progress': utils.EMPTY }\n",
    "    \n",
    "    cmd = utils.build_command(TOOL,parameters)\n",
    "    if is_very_fast :\n",
    "        cmd=cmd+SPACE+VERY_FAST # final command to run\n",
    "    \n",
    "    logging.info('Running nextgenmap with command:', cmd)\n",
    "    log_file=os.path.splitext(output_file) [0]+'.txt'\n",
    "    f=open(log_file, \"w\")\n",
    "    # run\n",
    "    try:\n",
    "        execStatus=subprocess.run(cmd.split(),stdout=f,stderr=f,check=True)\n",
    "        hdfs.copy_to_hdfs(log_file, LOG_DIR, overwrite=True)\n",
    "        if execStatus.returncode==0 and os.path.exists(output_file):\n",
    "             # copy output to hdfs\n",
    "            hdfs.copy_to_hdfs(output_file, OUTPUT_ROOT, overwrite=True)\n",
    "            # remove from local\n",
    "            os.remove(output_file)\n",
    "            return [True,output_file]\n",
    "    except subprocess.CalledProcessError:\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n",
    "        utils.hdfs_delete_file(os.path.join(OUTPUT_ROOT,output_file))\n",
    "        return False\n",
    "    finally:\n",
    "        f.close()\n",
    "        os.remove(filename_forward)\n",
    "        os.remove(filename_reverse)\n",
    "        if os.path.exists(log_file):\n",
    "            os.remove(log_file)\n",
    "        parameters.clear()\n",
    "\n",
    "    \n",
    "### single files\n",
    "def apply_ngm_single(x,REFERENCE_PATH):\n",
    "    \"\"\"\n",
    "    Runs NGM in single mode for single file.\n",
    "    Output is in SAM format and copied back to hdfs.\n",
    "    \"\"\"\n",
    "    \n",
    "    filename_forward_path=x\n",
    "    # split path to get file names\n",
    "    filename_forward=os.path.basename(filename_forward_path)\n",
    "    output_file=filename_forward.split('.')[0].replace(utils.R1,'')+'.sam'\n",
    "    # skip if output exists\n",
    "    if  utils.skip_file(filename_forward,output_file,OUTPUT_ROOT):\n",
    "         return [-1]\n",
    "    # install ngm\n",
    "    if IS_INSTALL_NGM and not os.path.exists(os.path.basename(TOOL_PATH)):\n",
    "        install_ngm(TOOL_PATH)\n",
    "    # get file name\n",
    "    ref=os.path.split(REFERENCE_PATH)[1]\n",
    "    if not os.path.exists(ref):\n",
    "        hdfs.copy_to_local(REFERENCE_PATH,overwrite=False)\n",
    "\n",
    "    hdfs.copy_to_local(filename_forward_path, overwrite=True)\n",
    "\n",
    "    parameters = { '-i':MIN_I, '-R': MIN_R, '-q': filename_forward, '-r': ref, \n",
    "                  '--silent-clip': utils.EMPTY, '-o': output_file, '-t': THREADS, '--no-progress': utils.EMPTY}\n",
    "    \n",
    "    cmd = utils.build_command(TOOL,parameters)\n",
    "    \n",
    "    if is_very_fast :\n",
    "        cmd=cmd+SPACE+VERY_FAST # final command to run\n",
    "    print(\"Running command:\", cmd)\n",
    "    #logging.info('Running nextgenmap with command:', cmd)\n",
    "    log_file=os.path.splitext(output_file) [0]+'.txt'\n",
    "    f=open(log_file, \"w\")\n",
    "    # run command\n",
    "    try:\n",
    "        execStatus=subprocess.run(cmd.split(),stdout=f,stderr=f,check=True)\n",
    "        \n",
    "        if execStatus.returncode==0 and os.path.exists(output_file):\n",
    "            # copy to hdfs\n",
    "            hdfs.copy_to_hdfs(output_file, OUTPUT_ROOT, overwrite=True)\n",
    "            # remove from local\n",
    "            os.remove(output_file)\n",
    "            return [True,output_file]\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n",
    "        utils.hdfs_delete_file(os.path.join(OUTPUT_ROOT,output_file))\n",
    "        return False\n",
    "    finally:\n",
    "        f.close()\n",
    "        parameters.clear()\n",
    "        os.remove(filename_forward)\n",
    "        if os.path.exists(log_file):\n",
    "            hdfs.copy_to_hdfs(log_file, LOG_DIR, overwrite=True)\n",
    "            os.remove(log_file)\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### List all input files hdfs path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_files=utils.load_file_names(INPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get list of all single end files and run NGM in single mode in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of single input  files processing  0"
     ]
    }
   ],
   "source": [
    "### single\n",
    "single_files=[f for f in all_files if utils.R_IDENTIFIER not in f]\n",
    "\n",
    "\n",
    "print('number of single input  files processing ', len(single_files))\n",
    "dataRdd=sc.parallelize(single_files,sc.getConf().get(\"spark.executor.instances\"))\n",
    "\n",
    "# run\n",
    "ngmSingleFiles=dataRdd.map(lambda x: apply_ngm_single(x,REFERENCE_PATH)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Pair R1 and R2 as a tuple in a list and run NGM in paired end in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs input files processing  1"
     ]
    }
   ],
   "source": [
    "\n",
    "# group r1 and r2\n",
    "pairedList = utils.group_R1R2(all_files)\n",
    "print('number of pairs input files processing ', len(pairedList))\n",
    "dataPairedRdd=sc.parallelize(pairedList,sc.getConf().get(\"spark.executor.instances\"))\n",
    "\n",
    "# run\n",
    "ngmFiles=dataPairedRdd.map(lambda x: apply_ngm_paired(x,REFERENCE_PATH) ).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}