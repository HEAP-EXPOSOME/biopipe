{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split large FASTQ files into smaller chunks\n",
    "\n",
    "Uses fastqsplitter python library.  Input files assumed are paired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from hops import hdfs\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "from pyspark import SparkContext,SparkConf\n",
    "import utils\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intilialise input and output folder and load input file hdfs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_full=utils.load_arguments(sys.argv)\n",
    "args=args_full[utils.KEY_SPLITFASTQ]\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    INPUT_ROOT=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "   INPUT_ROOT=args['INPUT_ROOT']\n",
    "\n",
    "all_files=utils.load_file_names(INPUT_ROOT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finds R1 and R2 total size for given pair.\n",
    "Returns array of pair(R1,R2) and total size.\n",
    "\"\"\"\n",
    "def get_pair_size(x):\n",
    "    r1=x[0] # r1 file name\n",
    "    r2=x[1] # r2 file name\n",
    "    r1_size=hdfs.stat(r1).st_size\n",
    "    r2_size=hdfs.stat(r2).st_size\n",
    "\n",
    "    return [x,r1_size+r2_size]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns string with names of splits of file depending on the number of splits\n",
    "\n",
    "eg. part1_sampleName.fq.gz part2_sampleName.fq.gz part3_sampleName.fq.gz\n",
    "\"\"\"\n",
    "def get_parts_file_names(file_path,number_of_splits,ext='.fastq.gz'):\n",
    "    file_name=os.path.basename(file_path)\n",
    "    name=os.path.splitext(os.path.splitext(file_name)[0])[0]\n",
    "    return ['part'+str(i)+'_'+name+ext for i in range(1,number_of_splits+1)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates number of splits based on file size and upper threshold.\n",
    "Default threshold = 4GB\n",
    "\n",
    "eg. For a file size of 8GB output is 2 with default threshold.\n",
    "\"\"\"\n",
    "def find_nbr_splits(size, threshold=4096000000):       \n",
    "    return math.ceil( size/threshold )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Splits single fastq file by calling fastqsplitter via subprocess.\n",
    "If successful, original input file is deleted.\n",
    "\"\"\"\n",
    "def split_file(input_file_path,files):\n",
    "    input_file_path=hdfs.get_plain_path(input_file_path)\n",
    "    print('Starting to split file: ', input_file_path)\n",
    "\n",
    "    hdfs.copy_to_local(input_file_path)\n",
    "    dirname, input_file = os.path.split(input_file_path)\n",
    "    output_args=' -o '+' -o '.join(files) # form the argument\n",
    "    cmd='fastqsplitter -i '+input_file+output_args\n",
    "    print('Running fastqsplitter with cmd: ', cmd)\n",
    "    \n",
    "    start=time.time()\n",
    "    try:\n",
    "        out=subprocess.run(cmd.split(' '),stdout=subprocess.PIPE)\n",
    "        end=time.time()\n",
    "        print( \" Processing time (sec):\" , end-start)\n",
    "        if out.returncode==0:\n",
    "            if not hdfs.exists(os.path.join(dirname,\"split\")):\n",
    "                hdfs.mkdir(os.path.join(dirname,\"split\"))\n",
    "            [ hdfs.copy_to_hdfs(x, os.path.join(dirname,\"split\"),overwrite=True) for x in files ]\n",
    "            [ os.remove(x) for x in files ]\n",
    "            return [input_file_path]\n",
    "    except (subprocess.CalledProcessError,OSError,IOError):\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        os.remove(input_file)\n",
    "\n",
    "    return  \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map function\n",
    "\n",
    "\n",
    "* Input is array of file name, size.\n",
    "* Depending on the size number of splits is calculated. Default threshold = 4GB as upper limit of file.\n",
    "* Depending on the number of parts the command argument is formed.\n",
    "* Finally fastqsplitter is called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Map function.\n",
    "Input is array of file name, size.\n",
    "Depending on the size numnber of splits is calculated.\n",
    "Depending on the number of parts the command argument is formed.\n",
    "Finally fastqsplitter is called.\n",
    "\"\"\"\n",
    "def mapSplit(input_pair):\n",
    "    input_file=input_pair[0]\n",
    "    size=input_pair[1]\n",
    "    nbr_parts=find_nbr_splits(size) # calculate number of splits based on size\n",
    "    files=get_parts_file_names(input_file,nbr_parts) # form arguments for splits\n",
    "    split_file(input_file,files) # split\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "* group R1 and R2 pairs\n",
    "* calculate each pair total size (R1+R2)\n",
    "* filter files based on total size and greater than specified threshold\n",
    "* flatten list to get separate entrieds for R1 and R2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into r1 and r2\n",
    "group=utils.group_R1R2(all_files)\n",
    "\n",
    "\n",
    "# get paired size\n",
    "paired_size=sc.parallelize(group).map(get_pair_size)\n",
    "\n",
    "\n",
    "# filter based on min size\n",
    "minSize=8192000000 # minimum size of file to select for spliting\n",
    "filtered_toSplit=paired_size.filter(lambda x: x[1] >= minSize)\n",
    "#filtered_toSplit=filtered_toSplit.map(lambda x : os.path.basename(x))\n",
    "paired_path=filtered_toSplit.collect()\n",
    "\n",
    "\n",
    "# separate r1 and r2\n",
    "flat_list=[]\n",
    "for i in paired_path:\n",
    "    flat_list.append((i[0][0],i[1])) # R1,total size (R1+R2)\n",
    "    flat_list.append((i[0][1],i[1])) # R2,total size (R1+R2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mapDeleteFile(hdfsPath):\n",
    "    if hdfsPath:\n",
    "        print('Splitting successful. Deleting original file at: ',hdfsPath)\n",
    "        hdfs.delete(hdfs.get_plain_path(hdfsPath))\n",
    "        return  [hdfsPath]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run flat list in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run split\n",
    "data=sc.parallelize(flat_list,sc.getConf().get(\"spark.executor.instances\"))\n",
    "split_files=data.map(mapSplit)\n",
    "split_files.collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete input files successfully split\n",
    "# take only file names\n",
    "if hdfs.exists(os.path.join(INPUT_ROOT,'split')):\n",
    "    splitted_files=utils.load_file_names(os.path.join(INPUT_ROOT,'split'))\n",
    "    inputFiles=[os.path.basename(f) for f in splitted_files]\n",
    "    # get unique sample name\n",
    "    uniques=utils.find_samples_with_lane(inputFiles)\n",
    "    print('#### Unique Samples to be deleted if exists ####')\n",
    "    utils.print_on_new_line(uniques)\n",
    "\n",
    "    # deleting files\n",
    "    deleted_files=[hdfs.delete(os.path.join(INPUT_ROOT,x)) for x in uniques if hdfs.exists(os.path.join(INPUT_ROOT,x))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}